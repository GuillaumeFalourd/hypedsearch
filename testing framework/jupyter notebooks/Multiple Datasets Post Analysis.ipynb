{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from pyteomics import fasta\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src import runner\n",
    "from src import gen_spectra\n",
    "from src.postprocessing import review\n",
    "from src import utils\n",
    "from src.scoring import scoring\n",
    "from src.objects import Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple datasets post analysis\n",
    "Run multiple data sets and do some post processing to see how well we do overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of datasets\n",
    "# make it a list of tuples of (mzml, spectrum mill *sv, database, prefix dir)\n",
    "Dataset = namedtuple(\n",
    "    'Dataset', \n",
    "    ['spectra_dir', 'spectrumMill_results', 'full_database', 'highest_dir', 'filtered_fasta']\n",
    ")\n",
    "\n",
    "\n",
    "raw_prefix = '/Users/zacharymcgrath/Desktop/raw inputs/'\n",
    "\n",
    "\n",
    "NOD2_data = Dataset(\n",
    "    raw_prefix + 'NOD2_E3/mzml/', \n",
    "    raw_prefix + 'NOD2_E3/NOD2_E3_results.ssv', \n",
    "    raw_prefix + 'mouse_database.fasta', \n",
    "    raw_prefix + 'NOD2_E3/', \n",
    "    ''\n",
    ")\n",
    "\n",
    "BALB3_data = Dataset(\n",
    "    raw_prefix + 'BALB3_E3/mzxml/', \n",
    "    raw_prefix + 'BALB3_E3/BALB3_E3.ssv', \n",
    "    raw_prefix + 'mouse_database.fasta',\n",
    "    raw_prefix + 'BALB3_E3/', \n",
    "    ''\n",
    ")\n",
    "\n",
    "datasets = [NOD2_data, BALB3_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data\n",
    "We don't care about the spectra that don't get matches in spectrumMill because we need something to compare to. Additionally, create a filtered database for just the proteins that are found in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_filter(db_file: str, results_file: str, output_fasta: str) -> None:\n",
    "    '''\n",
    "    Create the subset of proteins needed for the database search\n",
    "    \n",
    "    Inputs:\n",
    "        db_file:        (str)  the original fasta file\n",
    "        results_file:   (str)  the results ssv file from spectrumMill\n",
    "        output_fasta:   (str)  the fasta file to write to\n",
    "    '''\n",
    "    \n",
    "    # load all protiens into a dictionary\n",
    "    db = {}\n",
    "    for entry in fasta.read(db_file):\n",
    "        name = entry.description.split('|')[2]\n",
    "        name = name[:name.index('OS=')-1]\n",
    "        name = ' '.join(name.split(' ')[1:])\n",
    "        db[name.lower()] = entry\n",
    "\n",
    "    # load the results ssv into a dataframe \n",
    "    res_df = pd.read_csv(results_file, sep=';')\n",
    "        \n",
    "    print(f'Number of results: {len(res_df.index)}')\n",
    "\n",
    "    # keep track of those we want\n",
    "    filtered = []\n",
    "    for idx, row in res_df.iterrows():\n",
    "        key = row['entry_name'].lower()\n",
    "        \n",
    "        if key not in db:\n",
    "            continue\n",
    "            \n",
    "        filtered.append(db[key])\n",
    "\n",
    "    filtered = list(set(filtered))\n",
    "    \n",
    "    print(f'Number of proteins in database was reduced from {len(db)} to {len(filtered)}')\n",
    "    \n",
    "    fasta.write(filtered, output_fasta, file_mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_datasets = []\n",
    "\n",
    "for dataset in datasets:\n",
    "        \n",
    "    # make a file name for the output for the filtered fasta file\n",
    "    output_fasta = dataset.highest_dir + 'filtered_' + dataset.highest_dir.split('/')[-1].replace('/', '') + '_database.fasta'\n",
    "        \n",
    "    # check to see if we've created it before\n",
    "    if not os.path.isfile(output_fasta):\n",
    "        db_filter(dataset[2], dataset[1], output_fasta)\n",
    "    \n",
    "    updated_datasets.append(dataset._replace(filtered_fasta=output_fasta))\n",
    "\n",
    "datasets = updated_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>filename</th>\n",
       "      <th>parent_charge</th>\n",
       "      <th>score</th>\n",
       "      <th>deltaRank1Rank2Score</th>\n",
       "      <th>percent_scored_peak_intensity</th>\n",
       "      <th>num_unused_ions</th>\n",
       "      <th>totalIntensity</th>\n",
       "      <th>previous_aa</th>\n",
       "      <th>sequence</th>\n",
       "      <th>next_aa</th>\n",
       "      <th>sequenceMap</th>\n",
       "      <th>modifications</th>\n",
       "      <th>retentionTimeMin</th>\n",
       "      <th>chromatographicPeakWidthSec</th>\n",
       "      <th>parent_m_over_z</th>\n",
       "      <th>entry_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>BALB3_E3.14182.14222.2</td>\n",
       "      <td>2</td>\n",
       "      <td>14.28</td>\n",
       "      <td>11.12</td>\n",
       "      <td>89.1</td>\n",
       "      <td>6 / 25</td>\n",
       "      <td>263000.0</td>\n",
       "      <td>(G)</td>\n",
       "      <td>DLQTLALEVARA</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(G)D L|Q|T|L|A/L/E V A R A(D)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.35</td>\n",
       "      <td>28.0</td>\n",
       "      <td>650.3670</td>\n",
       "      <td>HYBRID: mouse ins1C QLELGGSPGDLQTLALEVAR-ADEFP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>BALB3_E3.13152.13328.2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.20</td>\n",
       "      <td>16.99</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0 / 25</td>\n",
       "      <td>27300000.0</td>\n",
       "      <td>(E)</td>\n",
       "      <td>DPQVAQLELGGGPGAGD</td>\n",
       "      <td>(L)</td>\n",
       "      <td>(E)D P Q V\\A\\Q\\L\\E|L|G|G|G|P G|A G D(L)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.13</td>\n",
       "      <td>41.0</td>\n",
       "      <td>790.8827</td>\n",
       "      <td>Insulin-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>341</td>\n",
       "      <td>BALB3_E3.15132.15175.3</td>\n",
       "      <td>3</td>\n",
       "      <td>13.15</td>\n",
       "      <td>4.51</td>\n",
       "      <td>89.3</td>\n",
       "      <td>5 / 25</td>\n",
       "      <td>912000.0</td>\n",
       "      <td>(-)</td>\n",
       "      <td>MLIKVKTLTGKEIEIDIEPT</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(-)M L|I K V K T L T G K E I E\\I D\\I|E|P/T(D)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.87</td>\n",
       "      <td>31.0</td>\n",
       "      <td>757.7679</td>\n",
       "      <td>NEDD8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>351</td>\n",
       "      <td>BALB3_E3.19192.19232.3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.52</td>\n",
       "      <td>2.71</td>\n",
       "      <td>80.9</td>\n",
       "      <td>8 / 25</td>\n",
       "      <td>483000.0</td>\n",
       "      <td>(K)</td>\n",
       "      <td>DISLSEYKGKYVVFFFYPL</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(K)D I|S\\L S E Y K G K Y V V F F F/Y|P L(D)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.32</td>\n",
       "      <td>10.0</td>\n",
       "      <td>772.4052</td>\n",
       "      <td>Peroxiredoxin-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>616</td>\n",
       "      <td>BALB3_E3.13192.13265.3</td>\n",
       "      <td>3</td>\n",
       "      <td>16.94</td>\n",
       "      <td>7.56</td>\n",
       "      <td>94.4</td>\n",
       "      <td>3 / 25</td>\n",
       "      <td>1020000.0</td>\n",
       "      <td>(-)</td>\n",
       "      <td>MRYVASYLLAALGGNSSPSAK</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(-)M R Y V A S Y\\L\\L\\A|A|L/G/G N S/S/P S A K(D)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.10</td>\n",
       "      <td>22.0</td>\n",
       "      <td>719.3777</td>\n",
       "      <td>60S acidic ribosomal protein P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>647</td>\n",
       "      <td>BALB3_E3.10112.10147.4</td>\n",
       "      <td>4</td>\n",
       "      <td>12.23</td>\n",
       "      <td>3.11</td>\n",
       "      <td>81.7</td>\n",
       "      <td>5 / 25</td>\n",
       "      <td>3370000.0</td>\n",
       "      <td>(Q)</td>\n",
       "      <td>DKALMRLPYGPGKSRANQIPKVAWIP</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(Q)D/K\\A L M R L P Y G P G K S R A N Q I\\P K V...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.60</td>\n",
       "      <td>17.0</td>\n",
       "      <td>727.4114</td>\n",
       "      <td>Secretogranin-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>695</td>\n",
       "      <td>BALB3_E3.12182.12328.2</td>\n",
       "      <td>2</td>\n",
       "      <td>14.06</td>\n",
       "      <td>8.62</td>\n",
       "      <td>83.5</td>\n",
       "      <td>4 / 25</td>\n",
       "      <td>4320000.0</td>\n",
       "      <td>(E)</td>\n",
       "      <td>EQYTPQSLATL</td>\n",
       "      <td>(E)</td>\n",
       "      <td>(E)E Q|Y\\T|P Q S\\L/A|T/L(E)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.87</td>\n",
       "      <td>25.0</td>\n",
       "      <td>625.8175</td>\n",
       "      <td>Secretogranin-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>880</td>\n",
       "      <td>BALB3_E3.10172.10212.3</td>\n",
       "      <td>3</td>\n",
       "      <td>13.02</td>\n",
       "      <td>6.95</td>\n",
       "      <td>81.6</td>\n",
       "      <td>10 / 25</td>\n",
       "      <td>754000.0</td>\n",
       "      <td>(I)</td>\n",
       "      <td>DWAYYRANVAKPGLV</td>\n",
       "      <td>(D)</td>\n",
       "      <td>(I)D W|A|Y/Y/R A N V A K/P G L V(D)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.77</td>\n",
       "      <td>15.0</td>\n",
       "      <td>574.9705</td>\n",
       "      <td>ATP synthase subunit d, mitochondrial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number                filename  parent_charge  score  \\\n",
       "9        10  BALB3_E3.14182.14222.2              2  14.28   \n",
       "83       84  BALB3_E3.13152.13328.2              2  20.20   \n",
       "340     341  BALB3_E3.15132.15175.3              3  13.15   \n",
       "350     351  BALB3_E3.19192.19232.3              3  11.52   \n",
       "615     616  BALB3_E3.13192.13265.3              3  16.94   \n",
       "646     647  BALB3_E3.10112.10147.4              4  12.23   \n",
       "694     695  BALB3_E3.12182.12328.2              2  14.06   \n",
       "879     880  BALB3_E3.10172.10212.3              3  13.02   \n",
       "\n",
       "     deltaRank1Rank2Score  percent_scored_peak_intensity num_unused_ions  \\\n",
       "9                   11.12                           89.1          6 / 25   \n",
       "83                  16.99                          100.0          0 / 25   \n",
       "340                  4.51                           89.3          5 / 25   \n",
       "350                  2.71                           80.9          8 / 25   \n",
       "615                  7.56                           94.4          3 / 25   \n",
       "646                  3.11                           81.7          5 / 25   \n",
       "694                  8.62                           83.5          4 / 25   \n",
       "879                  6.95                           81.6         10 / 25   \n",
       "\n",
       "     totalIntensity previous_aa                    sequence next_aa  \\\n",
       "9          263000.0         (G)                DLQTLALEVARA     (D)   \n",
       "83       27300000.0         (E)           DPQVAQLELGGGPGAGD     (L)   \n",
       "340        912000.0         (-)        MLIKVKTLTGKEIEIDIEPT     (D)   \n",
       "350        483000.0         (K)         DISLSEYKGKYVVFFFYPL     (D)   \n",
       "615       1020000.0         (-)       MRYVASYLLAALGGNSSPSAK     (D)   \n",
       "646       3370000.0         (Q)  DKALMRLPYGPGKSRANQIPKVAWIP     (D)   \n",
       "694       4320000.0         (E)                 EQYTPQSLATL     (E)   \n",
       "879        754000.0         (I)             DWAYYRANVAKPGLV     (D)   \n",
       "\n",
       "                                           sequenceMap  modifications  \\\n",
       "9                        (G)D L|Q|T|L|A/L/E V A R A(D)            NaN   \n",
       "83             (E)D P Q V\\A\\Q\\L\\E|L|G|G|G|P G|A G D(L)            NaN   \n",
       "340      (-)M L|I K V K T L T G K E I E\\I D\\I|E|P/T(D)            NaN   \n",
       "350        (K)D I|S\\L S E Y K G K Y V V F F F/Y|P L(D)            NaN   \n",
       "615    (-)M R Y V A S Y\\L\\L\\A|A|L/G/G N S/S/P S A K(D)            NaN   \n",
       "646  (Q)D/K\\A L M R L P Y G P G K S R A N Q I\\P K V...            NaN   \n",
       "694                        (E)E Q|Y\\T|P Q S\\L/A|T/L(E)            NaN   \n",
       "879                (I)D W|A|Y/Y/R A N V A K/P G L V(D)            NaN   \n",
       "\n",
       "     retentionTimeMin  chromatographicPeakWidthSec  parent_m_over_z  \\\n",
       "9               51.35                         28.0         650.3670   \n",
       "83              48.13                         41.0         790.8827   \n",
       "340             54.87                         31.0         757.7679   \n",
       "350             69.32                         10.0         772.4052   \n",
       "615             48.10                         22.0         719.3777   \n",
       "646             37.60                         17.0         727.4114   \n",
       "694             44.87                         25.0         625.8175   \n",
       "879             37.77                         15.0         574.9705   \n",
       "\n",
       "                                            entry_name  \n",
       "9    HYBRID: mouse ins1C QLELGGSPGDLQTLALEVAR-ADEFP...  \n",
       "83                                           Insulin-2  \n",
       "340                                              NEDD8  \n",
       "350                                    Peroxiredoxin-1  \n",
       "615                    60S acidic ribosomal protein P2  \n",
       "646                                    Secretogranin-2  \n",
       "694                                    Secretogranin-2  \n",
       "879              ATP synthase subunit d, mitochondrial  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('/Users/zacharymcgrath/Desktop/raw inputs/BALB3_E3/BALB3_E3.ssv', sep=';')\n",
    "d.loc[d['filename'].str.contains('BALB3_E3.1.1.2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run hyped search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database...\n",
      "Done\n",
      "Loading spectra...\n",
      "File /Users/zacharymcgrath/Desktop/raw inputs/NOD2_E3/mzml/.DS_Store is not of supported types (mzML, mzXML)\n",
      "Done\n",
      "On batch 1/1\n",
      "On protein 279/279 [100%]\n",
      "Sorting the set of protein masses...\n",
      "Done\n",
      "Initializing other processors...\n",
      "Done.\n",
      "Creating an alignment for 187/1086 [17%]"
     ]
    }
   ],
   "source": [
    "min_pep = 3\n",
    "max_pep = 30\n",
    "tolerance = 20\n",
    "precursor_tolerance = 10\n",
    "peak_filter = 25\n",
    "relative_abundance_filter = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "        \n",
    "    run_params = {\n",
    "        'spectra_folder': dataset.spectra_dir,\n",
    "        'database_file': dataset.filtered_fasta,\n",
    "        'output_dir': dataset.highest_dir + 'output/',\n",
    "        'min_peptide_len': min_pep,\n",
    "        'max_peptide_len': max_pep,\n",
    "        'tolerance': tolerance,\n",
    "        'precursor_tolerance': precursor_tolerance,\n",
    "        'peak_filter': peak_filter, \n",
    "        'relative_abundance_filter': relative_abundance_filter,\n",
    "        'digest': '', \n",
    "        'missed_cleavages': 0,\n",
    "        'verbose': True,\n",
    "        'DEBUG': False,\n",
    "        'cores': 16,\n",
    "        'truth_set': '', \n",
    "        'n': 5\n",
    "    }\n",
    "\n",
    "    runner.run(run_params)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep track of the output data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [\n",
    "    x[3] + 'output/summary.json' for x in datasets\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total analysis (both hybrids and non hybrids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each of the datasets, find out where the correct answer fell (no ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_table(df: pd.DataFrame, key: str, key_prepend: str = '') -> dict:\n",
    "    '''\n",
    "    Turn a pandas dataframe into a dictionary where the indices are the key specified\n",
    "    '''\n",
    "    ret_dict = {}\n",
    "    \n",
    "    prep = lambda k: key_prepend + str(k)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        ret_dict[prep(row[key])] = dict(row)\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(lambda: 0)\n",
    "\n",
    "individual_results = [defaultdict(lambda: 0) for _ in range(len(datasets))]\n",
    "\n",
    "nf_key = 'not found'\n",
    "na_key = 'no alignment'\n",
    "\n",
    "# keep trock of the ranks keyed by id\n",
    "id_keyed_results = {}\n",
    "\n",
    "for j, (dataset, output) in enumerate(zip(datasets, output_data)):\n",
    "    \n",
    "    nf = []\n",
    "    na = []\n",
    "    \n",
    "    # first we need to index the spectrumMill results by id\n",
    "    specMill_results = dictify_table(pd.read_csv(dataset[1], sep=';'), 'filename')\n",
    "    print(f'Total number of SpectrumMill results: {len(specMill_results)}')\n",
    "    \n",
    "    # now we need to load the output data\n",
    "    hypedSearch_results = json.load(open(output, 'r'))\n",
    "    print(f'Total number of hyped search results: {len(hypedSearch_results)}')\n",
    "        \n",
    "    # index hypedSearch_results by the id value in key, value value['spectrum']\n",
    "    \n",
    "    # for BALB3, we need other_metadata[0]['parentFileName']    \n",
    "    if 'BALB3' in dataset[1]:\n",
    "        hypedSearch_results = {value['spectrum']['other_metadata'][0]['parentFileName'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "    else:\n",
    "        hypedSearch_results = {value['spectrum']['id'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "        \n",
    "    # now go through every result in specMill, find the corresponding one in hypedSearch, and add to results\n",
    "    for key, value in specMill_results.items():\n",
    "        \n",
    "        hybrid = False\n",
    "        \n",
    "        if 'hybrid' in value['entry_name'].lower():\n",
    "            hybrid = True\n",
    "        \n",
    "        correct_sequence = value['sequence']\n",
    "        \n",
    "        \n",
    "        # if hybrid, replace L and I with B\n",
    "        if hybrid:\n",
    "            correct_sequence = correct_sequence.replace('L', 'B').replace('I', 'B')\n",
    "        \n",
    "        # look for it in the hypedSearch results\n",
    "        if key not in hypedSearch_results:\n",
    "            nf.append(key)\n",
    "            results[nf_key] += 1\n",
    "            individual_results[j][nf_key] += 1\n",
    "            id_keyed_results[key] = nf_key\n",
    "            continue \n",
    "            \n",
    "        found = False\n",
    "                    \n",
    "        for i, alignment in enumerate(hypedSearch_results[key]['alignments']):\n",
    "            \n",
    "            comp_sequence = alignment['sequence']\n",
    "            \n",
    "            # if hybrid, replace L and I with B\n",
    "            if hybrid:\n",
    "                comp_sequence = comp_sequence.replace('L', 'B').replace('I', 'B')\n",
    "            \n",
    "            if comp_sequence == correct_sequence:\n",
    "                results[str(i)] += 1\n",
    "                individual_results[j][str(i)] += 1\n",
    "                id_keyed_results[key] = i\n",
    "                found = True\n",
    "    \n",
    "        if not found:\n",
    "            na.append(key)\n",
    "            results[na_key] += 1\n",
    "            individual_results[j][na_key] += 1\n",
    "            id_keyed_results[key] = na_key\n",
    "    \n",
    "    print(f'{len(nf)} ids not found in hypedSearch results that were found in spectrumMill')\n",
    "    print(f'{len(na)} \"correct\" alignments from spectrumMill were not made in hyped search')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it by all results (no ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_int_keys = sorted([x for x in list(results.keys()) if x.isdigit()], key=lambda x: int(x))\n",
    "non_int_keys = [x for x in list(results.keys()) if not x.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "plt.bar(non_int_keys + sorted_int_keys, [results[k] for k in non_int_keys] + [results[k] for k in sorted_int_keys])\n",
    "plt.xlabel('Ranking of correct answer (no ties)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of results with a ranking for all datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(individual_results), 1, figsize=(20, 15*len(individual_results)))\n",
    "\n",
    "for i, ir in enumerate(individual_results):\n",
    "        \n",
    "    # first get the title\n",
    "    dataset_name = datasets[i].highest_dir.split('/')[-2]\n",
    "    title = f'Number of {dataset_name} results with ranking'\n",
    "    \n",
    "    # get the keys\n",
    "    ir_sik = sorted([x for x in list(ir.keys()) if x.isdigit()], key=lambda x: int(x))\n",
    "    ir_nik = [x for x in list(ir.keys()) if not x.isdigit()]\n",
    "    \n",
    "    axes[i].bar(ir_nik + ir_sik, [ir[k] for k in ir_nik] + [ir[k] for k in ir_sik])\n",
    "    \n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel('Ranking of correct answer (no ties)')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    \n",
    "    all_res = sum([v for _, v in ir.items()])\n",
    "    \n",
    "    print(f'{all_res} results for {dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each of the datasets, find out where the correct answer fell (with ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(lambda: 0)\n",
    "\n",
    "individual_results = [defaultdict(lambda: 0) for _ in range(len(datasets))]\n",
    "\n",
    "nf_key = 'not found'\n",
    "na_key = 'no alignment'\n",
    "\n",
    "for j, (dataset, output) in enumerate(zip(datasets, output_data)):\n",
    "    \n",
    "    nf = []\n",
    "    na = []\n",
    "    \n",
    "    # first we need to index the spectrumMill results by id\n",
    "    specMill_results = dictify_table(pd.read_csv(dataset[1], sep=';'), 'filename')\n",
    "    \n",
    "    # now we need to load the output data\n",
    "    hypedSearch_results = json.load(open(output, 'r'))\n",
    "    \n",
    "    # BALB dataset id found in another location\n",
    "    if 'BALB3' in dataset[1]:\n",
    "        hypedSearch_results = {value['spectrum']['other_metadata'][0]['parentFileName'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "        \n",
    "    else:\n",
    "        # index hypedSearch_results by the id value in key, value value['spectrum']\n",
    "        hypedSearch_results = {value['spectrum']['id'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "    \n",
    "    \n",
    "    # now go through every result in specMill, find the corresponding one in hypedSearch, and add to results\n",
    "    for key, value in specMill_results.items():\n",
    "        \n",
    "        hybrid = False\n",
    "        \n",
    "        if 'hybrid' in value['entry_name'].lower():\n",
    "            hybrid = True\n",
    "        \n",
    "        correct_sequence = value['sequence']\n",
    "        \n",
    "        # if hybrid, replace L and I with B\n",
    "        if hybrid:\n",
    "            correct_sequence = correct_sequence.replace('L', 'B').replace('I', 'B')\n",
    "        \n",
    "        # look for it in the hypedSearch results\n",
    "        if key not in hypedSearch_results:\n",
    "            nf.append(key)\n",
    "            results[nf_key] += 1\n",
    "            individual_results[j][nf_key] += 1\n",
    "            continue \n",
    "            \n",
    "        found = False\n",
    "        \n",
    "        # since we can allow for ties, we need to bin by the scores\n",
    "        scores = defaultdict(list)\n",
    "                    \n",
    "        for i, alignment in enumerate(hypedSearch_results[key]['alignments']):\n",
    "            \n",
    "            comp_sequence = alignment['sequence']\n",
    "            \n",
    "            # bin it \n",
    "            scores[alignment['total_score']].append(alignment)\n",
    "            \n",
    "            # if hybrid, replace L and I with B\n",
    "            if hybrid:\n",
    "                comp_sequence = comp_sequence.replace('L', 'B').replace('I', 'B')\n",
    "            \n",
    "            if comp_sequence == correct_sequence:\n",
    "                # the correct result rank is the len(scores) - 1\n",
    "                rank = str(len(scores) - 1)\n",
    "                \n",
    "                results[rank] += 1\n",
    "                individual_results[j][rank] += 1\n",
    "                found = True\n",
    "                \n",
    "        if not found:\n",
    "            na.append(key)\n",
    "            results[na_key] += 1\n",
    "            individual_results[j][na_key] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it by all results (with ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_int_keys = sorted([x for x in list(results.keys()) if x.isdigit()], key=lambda x: int(x))\n",
    "non_int_keys = [x for x in list(results.keys()) if not x.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "plt.bar(non_int_keys + sorted_int_keys, [results[k] for k in non_int_keys] + [results[k] for k in sorted_int_keys])\n",
    "plt.xlabel('Ranking of correct answer (no ties)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of results with a ranking for all datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(individual_results), 1, figsize=(20, 15*len(individual_results)))\n",
    "\n",
    "for i, ir in enumerate(individual_results):\n",
    "    \n",
    "    # first get the title\n",
    "    title = datasets[i].highest_dir.split('/')[-2]\n",
    "    title = f'Number of {title} results with ranking'\n",
    "    \n",
    "    # get the keys\n",
    "    ir_sik = sorted([x for x in list(ir.keys()) if x.isdigit()], key=lambda x: int(x))\n",
    "    ir_nik = [x for x in list(ir.keys()) if not x.isdigit()]\n",
    "    \n",
    "    axes[i].bar(ir_nik + ir_sik, [ir[k] for k in ir_nik] + [ir[k] for k in ir_sik])\n",
    "    \n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel('Ranking of correct answer (no ties)')\n",
    "    axes[i].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (dataset, output) in enumerate(zip(datasets, output_data)):\n",
    "    \n",
    "    missed = []\n",
    "    all_hybs = []\n",
    "    \n",
    "    print(f'Looking for hybrids in the dataset {dataset.highest_dir.split(\"/\")[-2]}\\n=====================================================')\n",
    "    \n",
    "    # first we need to index the spectrumMill results by id\n",
    "    specMill_results = dictify_table(pd.read_csv(dataset[1], sep=';'), 'filename')\n",
    "    \n",
    "    # now we need to load the output data\n",
    "    hypedSearch_results = json.load(open(output, 'r'))\n",
    "        \n",
    "    # index hypedSearch_results by the id value in key, value value['spectrum']\n",
    "    \n",
    "    # for BALB3, we need other_metadata[0]['parentFileName']    \n",
    "    if 'BALB3' in dataset[1]:\n",
    "        hypedSearch_results = {value['spectrum']['other_metadata'][0]['parentFileName'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "    else:\n",
    "        hypedSearch_results = {value['spectrum']['id'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "        \n",
    "    # now go through every result in specMill, find the corresponding one in hypedSearch, and add to results\n",
    "    for key, value in specMill_results.items():\n",
    "        \n",
    "        # skip it if not a hybrid\n",
    "        if not 'hybrid' in value['entry_name'].lower():\n",
    "            continue\n",
    "        \n",
    "        # replace with B for easier comparison\n",
    "        correct_sequence = value['sequence'].replace('L', 'B').replace('I', 'B')\n",
    "        \n",
    "        print(f'\"Correct\" hybrid sequence is {value[\"sequence\"]}\\n----------------------------------------')\n",
    "    \n",
    "        # look for it in the hypedSearch results\n",
    "        if key not in hypedSearch_results:\n",
    "            print(f'Alignment not made for {key}')\n",
    "            \n",
    "        found = False    \n",
    "        \n",
    "        # try to find a match for it \n",
    "        for i, alignment in enumerate(hypedSearch_results[key]['alignments']):\n",
    "            \n",
    "            print(f'{alignment[\"sequence\"]} \\t {alignment[\"total_score\"]} \\t {alignment[\"b_score\"]} \\t {alignment[\"y_score\"]} \\t {alignment[\"total_mass_error\"]}')\n",
    "            \n",
    "            if alignment[\"sequence\"].replace('L', 'B').replace('I', 'B') == correct_sequence:\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if not found:\n",
    "            missed.append(key)\n",
    "            \n",
    "        all_hybs.append(key)\n",
    "                \n",
    "        print()\n",
    "        \n",
    "    print(f'Missed {len(missed)}/{len(all_hybs)} hybrids for dataset {dataset.highest_dir.split(\"/\")[-2]}')\n",
    "    \n",
    "    print('\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass error distribution\n",
    "Go through all the ids. If the id had a rank of 0, skip it. We want to plot the distribution of total mass error for the OUR top alignment as well as the spectrumMill one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hypedSearch_rank_tme = []\n",
    "all_specMill_rank_tme = []\n",
    "\n",
    "individual_hs_rank_tme = []\n",
    "individual_sm_rank_tme = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    \n",
    "    i_hs_rank_tme = []\n",
    "    i_sm_rank_tme = []\n",
    "    \n",
    "     # first we need to index the spectrumMill results by id\n",
    "    specMill_results = dictify_table(pd.read_csv(dataset[1], sep=';'), 'filename')\n",
    "    \n",
    "    # now we need to load the output data\n",
    "    hypedSearch_results = json.load(open(output_data[i], 'r'))\n",
    "    \n",
    "    # for BALB3, we need other_metadata[0]['parentFileName']    \n",
    "    if 'BALB3' in dataset[1]:\n",
    "        hypedSearch_results = {value['spectrum']['other_metadata'][0]['parentFileName'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "    else:\n",
    "        hypedSearch_results = {value['spectrum']['id'].replace('.pkl', ''): value for _, value in hypedSearch_results.items()}\n",
    "        \n",
    "        \n",
    "    for _id, result in hypedSearch_results.items():\n",
    "        \n",
    "        skip_hs = not _id in id_keyed_results\n",
    "        skip_sm = not _id in specMill_results\n",
    "        \n",
    "            \n",
    "        # see if the id got a result of 0\n",
    "        if not skip_hs and id_keyed_results[_id] == 0:\n",
    "            continue\n",
    "            \n",
    "        # get the total mass error from hypedsearch. key the results by id, get the alignments\n",
    "        # at index from id_keyed_results, and get total_mass_error\n",
    "            \n",
    "        # if no alignment, skip the hypedSearch part\n",
    "        if not skip_hs:\n",
    "            rank = id_keyed_results[_id]\n",
    "            \n",
    "            if not (rank == nf_key or rank == na_key):\n",
    "                \n",
    "                all_hypedSearch_rank_tme.append((rank, hypedSearch_results[_id]['alignments'][rank]['total_mass_error']))\n",
    "                i_hs_rank_tme.append((rank, hypedSearch_results[_id]['alignments'][rank]['total_mass_error']))\n",
    "        \n",
    "        # we have the calculate the tme for spectrumMill results\n",
    "        if not skip_sm:\n",
    "            spec = Spectrum(**hypedSearch_results[_id]['spectrum'])\n",
    "            all_specMill_rank_tme.append((rank, scoring.total_mass_error(spec, specMill_results[_id]['sequence'], 20)))\n",
    "            i_sm_rank_tme.append((rank, scoring.total_mass_error(spec, specMill_results[_id]['sequence'], 20)))\n",
    "        \n",
    "    individual_hs_rank_tme.append(i_hs_rank_tme)\n",
    "    individual_sm_rank_tme.append(i_sm_rank_tme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of mass errors for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 12))\n",
    "\n",
    "axes[0].hist([x[1] for x in all_hypedSearch_rank_tme], bins=100)\n",
    "axes[0].set_title('hypedsearch')\n",
    "\n",
    "axes[1].hist([x[1] for x in all_specMill_rank_tme], bins=100)\n",
    "axes[1].set_title('SpectrumMill')\n",
    "\n",
    "fig.suptitle('Absolute mass error distributions when top results do not match between search tools')\n",
    "fig.text(0.5, 0.04, 'Absolute mass error', ha='center')\n",
    "fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of mass error per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = len(individual_hs_rank_tme)\n",
    "\n",
    "fig, axes = plt.subplots(num_row, 2, figsize=(20, 12 * num_row))\n",
    "\n",
    "fig.text(0.5, 0.04, 'Absolute mass error', ha='center')\n",
    "fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical')\n",
    "\n",
    "fig.suptitle('Absolute mass error distributions when top results do not match between search tools')\n",
    "\n",
    "for i, (hs_rank_tme, sm_rank_tme) in enumerate(zip(individual_hs_rank_tme, individual_sm_rank_tme)):\n",
    "\n",
    "    dataset_name = datasets[i].highest_dir.split('/')[-2]\n",
    "    \n",
    "    axes[i][0].hist([x[1] for x in hs_rank_tme], bins=100)\n",
    "    axes[i][0].set_title(f'hypedsearch on {dataset_name}')\n",
    "    \n",
    "    axes[i][1].hist([x[1] for x in sm_rank_tme], bins=100)\n",
    "    axes[i][1].set_title(f'SpectrumMill on {dataset_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
