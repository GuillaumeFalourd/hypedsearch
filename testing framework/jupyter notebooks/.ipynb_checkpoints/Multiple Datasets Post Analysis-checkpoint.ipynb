{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from pyteomics import fasta\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src import runner\n",
    "from src import gen_spectra\n",
    "from src.postprocessing import review\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple datasets post analysis\n",
    "Run multiple data sets and do some post processing to see how well we do overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of datasets\n",
    "# make it a list of tuples of (mzml, spectrum mill *sv, database, prefix dir)\n",
    "raw_prefix = '/Users/zacharymcgrath/Desktop/raw inputs/'\n",
    "NOD2_data = (\n",
    "    raw_prefix + 'NOD2_E3/NOD2_E3.mzML', \n",
    "    raw_prefix + 'NOD2_E3/NOD2_E3_results.ssv', \n",
    "    raw_prefix + 'mouse_database.fasta',\n",
    "    raw_prefix + 'NOD2_E3/'\n",
    ")\n",
    "\n",
    "BALB3_data = (\n",
    "    raw_prefix + 'BALB3_E3/BALB3_E3.mzML', \n",
    "    raw_prefix + 'BALB3_E3/BALB3_E3.ssv', \n",
    "    raw_prefix + 'mouse_database.fasta',\n",
    "    raw_prefix + 'BALB3_E3/'\n",
    ")\n",
    "\n",
    "datasets = [NOD2_data, BALB3_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data\n",
    "We don't care about the spectra that don't get matches in spectrumMill because we need something to compare to. Additionally, create a filtered database for just the proteins that are found in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_filter(db_file: str, results_file: str, output_fasta: str) -> None:\n",
    "    '''\n",
    "    Create the subset of proteins needed for the database search\n",
    "    \n",
    "    Inputs:\n",
    "        db_file:        (str)  the original fasta file\n",
    "        results_file:   (str)  the results ssv file from spectrumMill\n",
    "        output_fasta:   (str)  the fasta file to write to\n",
    "    '''\n",
    "    \n",
    "    # load all protiens into a dictionary\n",
    "    db = {}\n",
    "    for entry in fasta.read(db_file):\n",
    "        name = entry.description.split('|')[2]\n",
    "        name = name[:name.index('OS=')-1]\n",
    "        name = ' '.join(name.split(' ')[1:])\n",
    "        db[name.lower()] = entry\n",
    "\n",
    "    # load the results ssv into a dataframe \n",
    "    res_df = pd.read_csv(results_file, sep=';')\n",
    "    \n",
    "    print(f'Number of results: {len(res_df.index)}')\n",
    "\n",
    "    # keep track of those we want\n",
    "    filtered = []\n",
    "    for idx, row in res_df.iterrows():\n",
    "        key = row['entry_name'].lower()\n",
    "        \n",
    "        if key not in db:\n",
    "            continue\n",
    "            \n",
    "        filtered.append(db[key])\n",
    "\n",
    "    filtered = list(set(filtered))\n",
    "    \n",
    "    print(f'Number of proteins in database was reduced from {len(db)} to {len(filtered)}')\n",
    "    \n",
    "    fasta.write(filtered, output_fasta, file_mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 1086\n",
      "Number of proteins in database was reduced from 17028 to 279\n",
      "Number of results: 971\n",
      "Number of proteins in database was reduced from 17028 to 228\n"
     ]
    }
   ],
   "source": [
    "updated_datasets = []\n",
    "\n",
    "for dataset in datasets:\n",
    "        \n",
    "    # make a file name for the output\n",
    "    output_fasta = dataset[-1] + 'filtered_' + os.path.basename(dataset[1])\n",
    "    \n",
    "    db_filter(dataset[2], dataset[1], output_fasta)\n",
    "    \n",
    "    updated_datasets.append((*dataset, output_fasta))\n",
    "\n",
    "datasets = updated_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run hyped search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database...\n",
      "Done\n",
      "Loading spectra...\n",
      "Done\n",
      "On batch 1/1\n",
      "On protein 279/279 [100%]\n",
      "Sorting the set of protein masses...\n",
      "Done\n",
      "Initializing other processors...\n",
      "Done.\n",
      "Creating an alignment for 1085/1086 [100%]\n",
      "Finished search. Writting results to /Users/zacharymcgrath/Desktop/raw inputs/NOD2_E3/output/...\n",
      "Could not make an alignment for 1/1086 spectra (0%)\n",
      "Loading database...\n",
      "Done\n",
      "Loading spectra...\n",
      "Done\n",
      "On batch 1/1\n",
      "On protein 228/228 [100%]\n",
      "Sorting the set of protein masses...\n",
      "Done\n",
      "Initializing other processors...\n",
      "Done.\n",
      "Creating an alignment for 12165/12166 [100%]\n",
      "Finished search. Writting results to /Users/zacharymcgrath/Desktop/raw inputs/BALB3_E3/output/...\n",
      "Could not make an alignment for 1090/12166 spectra (8%)\n"
     ]
    }
   ],
   "source": [
    "min_pep = 3\n",
    "max_pep = 30\n",
    "tolerance = 20\n",
    "precursor_tolerance = 10\n",
    "peak_filter = 25\n",
    "relative_abundance_filter = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "        \n",
    "    run_params = {\n",
    "        'spectra_folder': dataset[3],\n",
    "        'database_file': dataset[-1],\n",
    "        'output_dir': dataset[3] + 'output/',\n",
    "        'min_peptide_len': min_pep,\n",
    "        'max_peptide_len': max_pep,\n",
    "        'tolerance': tolerance,\n",
    "        'precursor_tolerance': precursor_tolerance,\n",
    "        'peak_filter': peak_filter, \n",
    "        'relative_abundance_filter': relative_abundance_filter,\n",
    "        'digest': '', \n",
    "        'missed_cleavages': 0,\n",
    "        'verbose': True,\n",
    "        'DEBUG': False,\n",
    "        'cores': 16,\n",
    "        'truth_set': '', \n",
    "        'n': 100\n",
    "    }\n",
    "\n",
    "    runner.run(run_params)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep track of the output data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [\n",
    "    x[3] + 'output/summary.json' for x in datasets\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each of the datasets, find out where the correct answer fell (no ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictify_table(df: pd.DataFrame, key: str) -> dict:\n",
    "    '''\n",
    "    Turn a pandas dataframe into a dictionary where the indices are the key specified\n",
    "    '''\n",
    "    df.set_index(keys)\n",
    "    df.transpose()\n",
    "    return df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(lambda: 0)\n",
    "\n",
    "for dataset, output in zip(datasets, output_data):\n",
    "    \n",
    "    # first we need to index the spectrumMill results by id\n",
    "    specMill_results = dictify_table(pd.read_csv(dataset[1]), 'filename')\n",
    "    \n",
    "    # now we need to load the output data\n",
    "    hypedSearch_results = json.load(open(output_data, 'r'))\n",
    "    \n",
    "    # now go through every result in specMill, find the corresponding one in hypedSearch, and add to results\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
